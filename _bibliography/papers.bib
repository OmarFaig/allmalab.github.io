---
---

@inproceedings{ganesh2021,
    title={What Would a Teacher Do? Predicting Future Talk Moves},
    year={2021},
    author={Ananya Ganesh and Martha Palmer and Katharina Kann},
    booktitle = {Findings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{arocaouellette2021,
    title={PROST: Physical Reasoning of Objects through Space and Time},
    year={2021},
    author={Stephane Aroca-Ouellette and Cory Paik and Alessandro Roncone and Katharina Kann},
    booktitle = {Findings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{ebrahimi2021-1,
    title={How to Adapt Your Pretrained Multilingual Model to 1600 Languages},
    year={2021},
    author={Abteen Ebrahimi and Katharina Kann},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{bhatnagar2021,
    title={Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data},
    year={2021},
    author={Rajat Bhatnagar and Ananya Ganesh and Katharina Kann},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{mager2021,
    title = {Findings of the {A}mericas{NLP} 2021 Shared Task on Open Machine Translation for Indigenous Languages of the {A}mericas},
    author = {Mager, Manuel  and
      Oncevay, Arturo  and
      Ebrahimi, Abteen  and
      Ortega, John  and
      Rios, Annette  and
      Fan, Angela  and
      Gutierrez-Vasques, Ximena  and
      Chiruzzo, Luis  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Ramos, Ricardo  and
      Meza Ruiz, Ivan Vladimir  and
      Coto-Solano, Rolando  and
      Palmer, Alexis  and
      Mager-Hois, Elisabeth  and
      Chaudhary, Vishrav  and
      Neubig, Graham  and
      Vu, Ngoc Thang  and
      Kann, Katharina},
    year={2021},
    booktitle = "Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas",
    month = jun,
}

@inproceedings{kann_monsalve2021,
    title={Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings},
    author={Katharina Kann and Mauro M. Monsalve-Mercado},
    year={2021},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics",
    month = apr,
}

@inproceedings{beilei2021,
    title={CLiMP: A Benchmark for Chinese Language Model Evaluation},
    year={2021},
    author={Beilei Xiang and Changbing Yang and Yu Li and Alex Warstadt and Katharina Kann},
    booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics},
    month = apr,
}

@inproceedings{nikhil2020,
    title={Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies},
    author={Nikhil Prabhu and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing Student Research Workshop",
    month = dec,
    info={Best Paper Award}
}

@inproceedings{phang2020english,
    title={English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too},
    author={Jason Phang and Phu Mon Htut and Yada Pruksachatkun and Haokun Liu and Clara Vania and Iacer Calixto and Katharina Kann and Samuel R. Bowman},
    year={2020},
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing",
    month = dec,
}

@inproceedings{mager2020,
    title={Tackling the Low-resource Challenge for Canonical Segmentation},
    author={Manuel Mager and Özlem Çetinoğlu and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{agarwal2020,
    title={Acrostic Poem Generation},
    author={Rajat Agarwal and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{moeller2020,
    title={IGT2P: From Interlinear Glossed Texts to Paradigms},
    author={Sarah Moeller and Ling Liu and Changbing Yang and Katharina Kann and Mans Hulden},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{zhang-etal-2020-overfitting,
    title = "Why Overfitting Isn{'}t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries",
    author = "Zhang, Mozhi  and
      Fujinuma, Yoshinari  and
      Paul, Michael J.  and
      Boyd-Graber, Jordan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = {2020},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.201",
    doi = "10.18653/v1/2020.acl-main.201",
    pages = "2214--2220",
    abstract = "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.",
}

@inproceedings{kann-etal-2020-sigmorphon,
    title = "The {SIGMORPHON} 2020 Shared Task on Unsupervised Morphological Paradigm Completion",
    author = "Kann, Katharina  and
      McCarthy, Arya D.  and
      Nicolai, Garrett  and
      Hulden, Mans",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.3",
    doi = "10.18653/v1/2020.sigmorphon-1.3",
    pages = "51--62",
    abstract = "In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology. Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma. In order to simulate a realistic use case, we first released data for 5 development languages. However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline. We provided a modular baseline system, which is a pipeline of 4 components. 3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic.",
}

@inproceedings{prabhu-kann-2020-frustratingly,
    title = "Frustratingly Easy Multilingual Grapheme-to-Phoneme Conversion",
    author = "Prabhu, Nikhil  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.13",
    doi = "10.18653/v1/2020.sigmorphon-1.13",
    pages = "123--127",
    abstract = "In this paper, we describe two CU-Boulder submissions to the SIGMORPHON 2020 Task 1 on multilingual grapheme-to-phoneme conversion (G2P). Inspired by the high performance of a standard transformer model (Vaswani et al., 2017) on the task, we improve over this approach by adding two modifications: (i) Instead of training exclusively on G2P, we additionally create examples for the opposite direction, phoneme-to-grapheme conversion (P2G). We then perform multi-task training on both tasks. (ii) We produce ensembles of our models via majority voting. Our approaches, though being conceptually simple, result in systems that place 6th and 8th amongst 23 submitted systems, and obtain the best results out of all systems on Lithuanian and Modern Greek, respectively.",
}

@inproceedings{singer-kann-2020-nyu,
    title = "The {NYU}-{CUB}oulder Systems for {SIGMORPHON} 2020 Task 0 and Task 2",
    author = "Singer, Assaf  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.8",
    doi = "10.18653/v1/2020.sigmorphon-1.8",
    pages = "90--98",
    abstract = "We describe the NYU-CUBoulder systems for the SIGMORPHON 2020 Task 0 on typologically diverse morphological inflection and Task 2 on unsupervised morphological paradigm completion. The former consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. The latter requires generating entire paradigms for a set of given lemmas from raw text alone. We model morphological inflection as a sequence-to-sequence problem, where the input is the sequence of the lemma{'}s characters with morphological tags, and the output is the sequence of the inflected form{'}s characters. First, we apply a transformer model to the task. Second, as inflected forms share most characters with the lemma, we further propose a pointer-generator transformer model to allow easy copying of input characters.",
}

@inproceedings{mager-kann-2020-ims,
    title = "The {IMS}{--}{CUB}oulder System for the {SIGMORPHON} 2020 Shared Task on Unsupervised Morphological Paradigm Completion",
    author = "Mager, Manuel  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.9",
    doi = "10.18653/v1/2020.sigmorphon-1.9",
    pages = "99--105",
    abstract = "In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMS--CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.",
}


@inproceedings{mohananey-etal-2020-self,
    title = "Self-Training for Unsupervised Parsing with {PRPN}",
    author = "Mohananey, Anhad  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.iwpt-1.11",
    doi = "10.18653/v1/2020.iwpt-1.11",
    pages = "105--110",
    abstract = "Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like language modeling. In this work, we propose self-training for neural UP models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. To be able to use our model{'}s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with parses predicted by our model to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1{\%} F1 and the previous state of the art by 1.6{\%} F1. In addition, we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings.",
}

@inproceedings{pruksachatkun-etal-2020-intermediate,
    title = "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
    author = "Pruksachatkun, Yada  and
      Phang, Jason  and
      Liu, Haokun  and
      Htut, Phu Mon  and
      Zhang, Xiaoyi  and
      Pang, Richard Yuanzhe  and
      Vania, Clara  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.467",
    doi = "10.18653/v1/2020.acl-main.467",
    pages = "5231--5247",
    abstract = "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
}

@inproceedings{jin-etal-2020-unsupervised,
    title = "Unsupervised Morphological Paradigm Completion",
    author = "Jin, Huiming  and
      Cai, Liwei  and
      Peng, Yihui  and
      Xia, Chen  and
      McCarthy, Arya  and
      Kann, Katharina",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.598",
    doi = "10.18653/v1/2020.acl-main.598",
    pages = "6696--6707",
    abstract = "We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.",
}

@inproceedings{kann2020learning,
    title={Learning to Learn Morphological Inflection for Resource-Poor Languages},
    author={Katharina Kann and Samuel R. Bowman and Kyunghyun Cho},
    year={2020},
    booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence}
}

@inproceedings{kann2020weakly,
    title={Weakly Supervised POS Taggers Perform Poorly on Truly Low-Resource Languages},
    author={Katharina Kann and Ophélie Lacroix and Anders Søgaard},
    year={2020},
    booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence}
}

@inproceedings{kann-2020-acquisition,
    title = "Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge",
    author = "Kann, Katharina",
    booktitle = "Proceedings of the Society for Computation in Linguistics",
    month = jan,
    year = "2020",
    address = "New York, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.scil-1.19",
    pages = "129--138",
}
