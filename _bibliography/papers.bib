---
---
@ARTICLE{kann2022frontiers,
    AUTHOR={Kann, Katharina and Ebrahimi, Abteen and Mager, Manuel and Oncevay, Arturo and Ortega, John E. and Rios, Annette and Fan, Angela and Gutierrez-Vasques, Ximena and Chiruzzo, Luis and Giménez-Lugo, Gustavo A. and Ramos, Ricardo and Meza Ruiz, Ivan Vladimir and Mager, Elisabeth and Chaudhary, Vishrav and Neubig, Graham and Palmer, Alexis and Coto-Solano, Rolando and Vu, Ngoc Thang},
    TITLE={AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas},
    JOURNAL={Frontiers in Artificial Intelligence},
    VOLUME={5},
    YEAR={2022},
    MONTH={DECEMBER},
    URL={https://www.frontiersin.org/articles/10.3389/frai.2022.995667},
    DOI={10.3389/frai.2022.995667},
    ISSN={2624-8212},
    ABSTRACT={Little attention has been paid to the development of human language technology for truly low-resource languages—i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.}
}


@inproceedings{kann2022emnlp,
    title = "A Major Obstacle for NLP Research: Let's Talk about Time Allocation!",
    author = "Kann, Katharina and Dudy, Shiran and McCarthy, Arya D.",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2022",
}

@inproceedings{wiemerslage2022emnlp,
    title = "A Comprehensive Comparison of Neural Networks as Cognitive Models of Inflection",
    author = "Wiemerslage, Adam and Dudy, Shiran and Kann, Katharina",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2022",
}

@inproceedings{bhatnagar2022emnlp,
    title = "CHIA: CHoosing Instances to Annotate for Machine Translation",
    author = "Bhatnagar, Rajat and Ganesh, Ananya and Kann, Katharina",
    booktitle = "Findings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = december,
    year = "2022",
}

@inproceedings{hall2022emnlp,
    title = "Generate Me a Bedtime Story: Leveraging Natural Language Processing for Early Vocabulary Enhancement",
    author = "Hall, Trevor A. and Valentini, Maria and Colunga, Eliana and Kann, Katharina",
    booktitle = "Proceedings of the Workshop on NLP for Positive Impact",
    month = december,
    year = "2022",
}

@inproceedings{kannfieldmatters2022,
    title={Machine Translation Between High-resource Languages in a Language Documentation Setting},
    year={2022},
    author={Kann, Katharina and Ebrahimi, Abteen and Stenzel, Kristine and Palmer, Alexis},
    booktitle = {Proceedings of the First Workshop on Applying NLP to Field Linguistics},
    month = october,
}

@inproceedings{ganesh2022,
    title={Response Construct Tagging: NLP-Aided Assessment for Engineering Education},
    year={2022},
    author={Ganesh, Ananya and Scribner, Hugh and Singh, Jasdeep and Goodman, Katherine and Hertzberg, Jean and Kann, Katharina},
    booktitle = {Proceedings of the 17th Workshop on Innovative Use of NLP for Building Educational Applications},
    month = july,
}

@inproceedings{kann2022,
    title={Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next},
    year={2022},
    author={Kann, Katharina and Ebrahimi, Abteen and Koh, Joewie J. and Dudy, Shiran and Roncone, Alessandro},
    booktitle = {Proceedings of the 4th Workshop on NLP for Conversational AI},
    month = may,
    poster = {Posters/2022/ConvAI/Katharina.pdf}
}

@inproceedings{ebrahimi2022,
    title={AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages},
    year={2022},
    author={Ebrahimi, Abteen and Mager, Manuel and Oncevay, Arturo and Chaudhary, Vishrav and Chiruzzo, Luis and Fan, Angela and Ortega, John and Ramos, Ricardo and Rios, Annette and Meza Ruiz, Ivan Vladimir and Giménez-Lugo, Gustavo A. and Mager, Elisabeth and Neubig, Graham and Palmer, Alexis and  Coto-Solano, Rolando and Vu, Thang and Kann, Katharina},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may,
    video = {/assets/videos/2022/ACL/Abteen.mp4}
}

@inproceedings{fujinuma2022,
    title={How Does Multilingual Pretraining Affect Cross-Lingual Transferability?},
    year={2022},
    author={Fujinuma, Yoshinari and Boyd-Graber, Jordan Lee and Kann, Katharina},
    booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may
}

@inproceedings{wiemerslage2022,
    title={Morphological Processing of Low-Resource Languages: Where We Are and What's Next},
    year={2022},
    author={Wiemerslage, Adam and Silfverberg, Miikka and Yang, Changbing and McCarthy, Arya D. and Nicolai, Garrett and Colunga, Eliana and Kann, Katharina},
    booktitle = {Findings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may,
    video = {/assets/videos/2022/ACL/adam.mp4},
    poster = {Posters/2022/ACL/Adam.pdf}
}

@inproceedings{mager2022,
    title={BPE vs. Morphological Segmentation: A Case Study on Machine Translation of Four Polysynthetic Languages},
    year={2022},
    author={Mager, Manuel and Oncevay, Arturo and Mager, Elisabeth and Kann, Katharina and Vu, Thang},
    booktitle = {Findings of the 60th Annual Meeting of the Association for Computational Linguistics},
    month = may,
}

@inproceedings{paik-etal-2021-world,
    title = "{T}he {W}orld of an {O}ctopus: {H}ow {R}eporting {B}ias {I}nfluences a {L}anguage {M}odel{'}s {P}erception of {C}olor",
    author = "Paik, Cory  and
      Aroca-Ouellette, St{\'e}phane  and
      Roncone, Alessandro  and
      Kann, Katharina",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.63",
    doi = "10.18653/v1/2021.emnlp-main.63",
    pages = "823--835",
    abstract = "Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human{'}s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.",
}

@inproceedings{ganesh2021,
    title={What Would a Teacher Do? Predicting Future Talk Moves},
    year={2021},
    author={Ananya Ganesh and Martha Palmer and Katharina Kann},
    booktitle = {Findings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{arocaouellette2021,
    title={PROST: Physical Reasoning of Objects through Space and Time},
    year={2021},
    author={Stephane Aroca-Ouellette and Cory Paik and Alessandro Roncone and Katharina Kann},
    booktitle = {Findings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{ebrahimi2021-1,
    title={How to Adapt Your Pretrained Multilingual Model to 1600 Languages},
    year={2021},
    author={Abteen Ebrahimi and Katharina Kann},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{bhatnagar2021,
    title={Don't Rule Out Monolingual Speakers: A Method For Crowdsourcing Machine Translation Data},
    year={2021},
    author={Rajat Bhatnagar and Ananya Ganesh and Katharina Kann},
    booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
    month = aug,
}

@inproceedings{ojha-etal-2021-findings,
    title = "Findings of the {L}o{R}es{MT} 2021 Shared Task on {COVID} and Sign Language for Low-resource Languages",
    author = "Ojha, Atul Kr.  and
      Liu, Chao-Hong  and
      Kann, Katharina  and
      Ortega, John  and
      Shatam, Sheetal  and
      Fransen, Theodorus",
    booktitle = "Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021)",
    month = aug,
    year = "2021",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2021.mtsummit-loresmt.11",
    pages = "114--123",
    abstract = "We present the findings of the LoResMT 2021 shared task which focuses on machine translation (MT) of COVID-19 data for both low-resource spoken and sign languages. The organization of this task was conducted as part of the fourth workshop on technologies for machine translation of low resource languages (LoResMT). Parallel corpora is presented and publicly available which includes the following directions: English↔Irish, English↔Marathi, and Taiwanese Sign language↔Traditional Chinese. Training data consists of 8112, 20933 and 128608 segments, respectively. There are additional monolingual data sets for Marathi and English that consist of 21901 segments. The results presented here are based on entries from a total of eight teams. Three teams submitted systems for English↔Irish while five teams submitted systems for English↔Marathi. Unfortunately, there were no systems submissions for the Taiwanese Sign language↔Traditional Chinese task. Maximum system performance was computed using BLEU and follow as 36.0 for English{--}Irish, 34.6 for Irish{--}English, 24.2 for English{--}Marathi, and 31.3 for Marathi{--}English.",
}

@inproceedings{gerlach-etal-2021-paradigm,
    title = "Paradigm Clustering with Weighted Edit Distance",
    author = "Gerlach, Andrew  and
      Wiemerslage, Adam  and
      Kann, Katharina",
    booktitle = "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigmorphon-1.12",
    doi = "10.18653/v1/2021.sigmorphon-1.12",
    pages = "107--114",
    abstract = "This paper describes our system for the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering, which asks participants to group inflected forms together according their underlying lemma without the aid of annotated training data. We employ agglomerative clustering to group word forms together using a metric that combines an orthographic distance and a semantic distance from word embeddings. We experiment with two variations of an edit distance-based model for quantifying orthographic distance, but, due to time constraints, our system does not improve over the shared task{'}s baseline system.",
}

@inproceedings{wiemerslage-etal-2021-findings,
    title = "Findings of the {SIGMORPHON} 2021 Shared Task on Unsupervised Morphological Paradigm Clustering",
    author = "Wiemerslage, Adam  and
      McCarthy, Arya D.  and
      Erdmann, Alexander  and
      Nicolai, Garrett  and
      Agirrezabal, Manex  and
      Silfverberg, Miikka  and
      Hulden, Mans  and
      Kann, Katharina",
    booktitle = "Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.sigmorphon-1.8",
    doi = "10.18653/v1/2021.sigmorphon-1.8",
    pages = "72--81",
    abstract = "We describe the second SIGMORPHON shared task on unsupervised morphology: the goal of the SIGMORPHON 2021 Shared Task on Unsupervised Morphological Paradigm Clustering is to cluster word types from a raw text corpus into paradigms. To this end, we release corpora for 5 development and 9 test languages, as well as gold partial paradigms for evaluation. We receive 14 submissions from 4 teams that follow different strategies, and the best performing system is based on adaptor grammars. Results vary significantly across languages. However, all systems are outperformed by a supervised lemmatizer, implying that there is still room for improvement.",
}

@inproceedings{mager2021,
    title = {Findings of the {A}mericas{NLP} 2021 Shared Task on Open Machine Translation for Indigenous Languages of the {A}mericas},
    author = {Mager, Manuel  and
      Oncevay, Arturo  and
      Ebrahimi, Abteen  and
      Ortega, John  and
      Rios, Annette  and
      Fan, Angela  and
      Gutierrez-Vasques, Ximena  and
      Chiruzzo, Luis  and
      Gim{\'e}nez-Lugo, Gustavo  and
      Ramos, Ricardo  and
      Meza Ruiz, Ivan Vladimir  and
      Coto-Solano, Rolando  and
      Palmer, Alexis  and
      Mager-Hois, Elisabeth  and
      Chaudhary, Vishrav  and
      Neubig, Graham  and
      Vu, Ngoc Thang  and
      Kann, Katharina},
    year={2021},
    booktitle = "Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas",
    month = jun,
}

@inproceedings{kann_monsalve2021,
    title={Coloring the Black Box: What Synesthesia Tells Us about Character Embeddings},
    author={Katharina Kann and Mauro M. Monsalve-Mercado},
    year={2021},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics",
    month = apr,
}

@inproceedings{beilei2021,
    title={CLiMP: A Benchmark for Chinese Language Model Evaluation},
    year={2021},
    author={Beilei Xiang and Changbing Yang and Yu Li and Alex Warstadt and Katharina Kann},
    booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics},
    month = apr,
}

@inproceedings{nikhil2020,
    title={Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies},
    author={Nikhil Prabhu and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing Student Research Workshop",
    month = dec,
    info={Best Paper Award}
}

@inproceedings{phang2020english,
    title={English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too},
    author={Jason Phang and Phu Mon Htut and Yada Pruksachatkun and Haokun Liu and Clara Vania and Iacer Calixto and Katharina Kann and Samuel R. Bowman},
    year={2020},
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 9th International Joint Conference on Natural Language Processing",
    month = dec,
}

@inproceedings{mager2020,
    title={Tackling the Low-resource Challenge for Canonical Segmentation},
    author={Manuel Mager and Özlem Çetinoğlu and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{agarwal2020,
    title={Acrostic Poem Generation},
    author={Rajat Agarwal and Katharina Kann},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{moeller2020,
    title={IGT2P: From Interlinear Glossed Texts to Paradigms},
    author={Sarah Moeller and Ling Liu and Changbing Yang and Katharina Kann and Mans Hulden},
    year={2020},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
}

@inproceedings{zhang-etal-2020-overfitting,
    title = "Why Overfitting Isn{'}t Always Bad: Retrofitting Cross-Lingual Word Embeddings to Dictionaries",
    author = "Zhang, Mozhi  and
      Fujinuma, Yoshinari  and
      Paul, Michael J.  and
      Boyd-Graber, Jordan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = {2020},
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.201",
    doi = "10.18653/v1/2020.acl-main.201",
    pages = "2214--2220",
    abstract = "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI). Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI. However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary. We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary. This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy. We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks. Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.",
}

@inproceedings{kann-etal-2020-sigmorphon,
    title = "The {SIGMORPHON} 2020 Shared Task on Unsupervised Morphological Paradigm Completion",
    author = "Kann, Katharina  and
      McCarthy, Arya D.  and
      Nicolai, Garrett  and
      Hulden, Mans",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.3",
    doi = "10.18653/v1/2020.sigmorphon-1.3",
    pages = "51--62",
    abstract = "In this paper, we describe the findings of the SIGMORPHON 2020 shared task on unsupervised morphological paradigm completion (SIGMORPHON 2020 Task 2), a novel task in the field of inflectional morphology. Participants were asked to submit systems which take raw text and a list of lemmas as input, and output all inflected forms, i.e., the entire morphological paradigm, of each lemma. In order to simulate a realistic use case, we first released data for 5 development languages. However, systems were officially evaluated on 9 surprise languages, which were only revealed a few days before the submission deadline. We provided a modular baseline system, which is a pipeline of 4 components. 3 teams submitted a total of 7 systems, but, surprisingly, none of the submitted systems was able to improve over the baseline on average over all 9 test languages. Only on 3 languages did a submitted system obtain the best results. This shows that unsupervised morphological paradigm completion is still largely unsolved. We present an analysis here, so that this shared task will ground further research on the topic.",
}

@inproceedings{prabhu-kann-2020-frustratingly,
    title = "Frustratingly Easy Multilingual Grapheme-to-Phoneme Conversion",
    author = "Prabhu, Nikhil  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.13",
    doi = "10.18653/v1/2020.sigmorphon-1.13",
    pages = "123--127",
    abstract = "In this paper, we describe two CU-Boulder submissions to the SIGMORPHON 2020 Task 1 on multilingual grapheme-to-phoneme conversion (G2P). Inspired by the high performance of a standard transformer model (Vaswani et al., 2017) on the task, we improve over this approach by adding two modifications: (i) Instead of training exclusively on G2P, we additionally create examples for the opposite direction, phoneme-to-grapheme conversion (P2G). We then perform multi-task training on both tasks. (ii) We produce ensembles of our models via majority voting. Our approaches, though being conceptually simple, result in systems that place 6th and 8th amongst 23 submitted systems, and obtain the best results out of all systems on Lithuanian and Modern Greek, respectively.",
}

@inproceedings{singer-kann-2020-nyu,
    title = "The {NYU}-{CUB}oulder Systems for {SIGMORPHON} 2020 Task 0 and Task 2",
    author = "Singer, Assaf  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.8",
    doi = "10.18653/v1/2020.sigmorphon-1.8",
    pages = "90--98",
    abstract = "We describe the NYU-CUBoulder systems for the SIGMORPHON 2020 Task 0 on typologically diverse morphological inflection and Task 2 on unsupervised morphological paradigm completion. The former consists of generating morphological inflections from a lemma and a set of morphosyntactic features describing the target form. The latter requires generating entire paradigms for a set of given lemmas from raw text alone. We model morphological inflection as a sequence-to-sequence problem, where the input is the sequence of the lemma{'}s characters with morphological tags, and the output is the sequence of the inflected form{'}s characters. First, we apply a transformer model to the task. Second, as inflected forms share most characters with the lemma, we further propose a pointer-generator transformer model to allow easy copying of input characters.",
}

@inproceedings{mager-kann-2020-ims,
    title = "The {IMS}{--}{CUB}oulder System for the {SIGMORPHON} 2020 Shared Task on Unsupervised Morphological Paradigm Completion",
    author = "Mager, Manuel  and
      Kann, Katharina",
    booktitle = "Proceedings of the 17th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.sigmorphon-1.9",
    doi = "10.18653/v1/2020.sigmorphon-1.9",
    pages = "99--105",
    abstract = "In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMS--CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.",
}


@inproceedings{mohananey-etal-2020-self,
    title = "Self-Training for Unsupervised Parsing with {PRPN}",
    author = "Mohananey, Anhad  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task on Parsing into Enhanced Universal Dependencies",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.iwpt-1.11",
    doi = "10.18653/v1/2020.iwpt-1.11",
    pages = "105--110",
    abstract = "Neural unsupervised parsing (UP) models learn to parse without access to syntactic annotations, while being optimized for another task like language modeling. In this work, we propose self-training for neural UP models: we leverage aggregated annotations predicted by copies of our model as supervision for future copies. To be able to use our model{'}s predictions during training, we extend a recent neural UP architecture, the PRPN (Shen et al., 2018a), such that it can be trained in a semi-supervised fashion. We then add examples with parses predicted by our model to our unlabeled UP training data. Our self-trained model outperforms the PRPN by 8.1{\%} F1 and the previous state of the art by 1.6{\%} F1. In addition, we show that our architecture can also be helpful for semi-supervised parsing in ultra-low-resource settings.",
}

@inproceedings{pruksachatkun-etal-2020-intermediate,
    title = "Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?",
    author = "Pruksachatkun, Yada  and
      Phang, Jason  and
      Liu, Haokun  and
      Htut, Phu Mon  and
      Zhang, Xiaoyi  and
      Pang, Richard Yuanzhe  and
      Vania, Clara  and
      Kann, Katharina  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.467",
    doi = "10.18653/v1/2020.acl-main.467",
    pages = "5231--5247",
    abstract = "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
}

@inproceedings{jin-etal-2020-unsupervised,
    title = "Unsupervised Morphological Paradigm Completion",
    author = "Jin, Huiming  and
      Cai, Liwei  and
      Peng, Yihui  and
      Xia, Chen  and
      McCarthy, Arya  and
      Kann, Katharina",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.598",
    doi = "10.18653/v1/2020.acl-main.598",
    pages = "6696--6707",
    abstract = "We propose the task of unsupervised morphological paradigm completion. Given only raw text and a lemma list, the task consists of generating the morphological paradigms, i.e., all inflected forms, of the lemmas. From a natural language processing (NLP) perspective, this is a challenging unsupervised task, and high-performing systems have the potential to improve tools for low-resource languages or to assist linguistic annotators. From a cognitive science perspective, this can shed light on how children acquire morphological knowledge. We further introduce a system for the task, which generates morphological paradigms via the following steps: (i) EDIT TREE retrieval, (ii) additional lemma retrieval, (iii) paradigm size discovery, and (iv) inflection generation. We perform an evaluation on 14 typologically diverse languages. Our system outperforms trivial baselines with ease and, for some languages, even obtains a higher accuracy than minimally supervised systems.",
}

@inproceedings{kann2020learning,
    title={Learning to Learn Morphological Inflection for Resource-Poor Languages},
    author={Katharina Kann and Samuel R. Bowman and Kyunghyun Cho},
    year={2020},
    booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence}
}

@inproceedings{kann2020weakly,
    title={Weakly Supervised POS Taggers Perform Poorly on Truly Low-Resource Languages},
    author={Katharina Kann and Ophélie Lacroix and Anders Søgaard},
    year={2020},
    booktitle = {Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence}
}

@inproceedings{kann-2020-acquisition,
    title = "Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge",
    author = "Kann, Katharina",
    booktitle = "Proceedings of the Society for Computation in Linguistics",
    month = jan,
    year = "2020",
    address = "New York, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.scil-1.19",
    pages = "129--138",
}
