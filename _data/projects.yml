- name: Encoder-only foundation models for Azerbaijani
  description: We are working to build foundation models for Azerbaijani. These models are encoder-only, which makes them suitable for various NLU tasks. First iteration of the project has been completed. We present our [results](https://aclanthology.org/2024.sigturk-1.2/) in ACL 2024, as part of the 1st SIGTURK Workshop. Our models are publicly hosted on [Hugging Face](https://huggingface.co/collections/allmalab/allma-models-669b63077f28c43e091270cb). As a part of the project, we have also released [a text corpus](https://huggingface.co/datasets/allmalab/DOLLMA), a [tokenizer](https://huggingface.co/allmalab/bert-tokenizer-aze) and several evaluation datasets for Azerbaini.
  link: foundation_models
  
- name: Semantic embedding models for Azerbaijani
  description: Building a foundation model does not necessarily give us a semantic embedding model. We may need to fine-tune these models further for this task. This has been quite tricky for Azerbaijani.
  link: embeddings
  
- name: Web-scale text corpus for Azerbaijani
  description: There are several Azerbaijani text corpora at the scale of hundreds of millions of words. We intend to push this number to billions without sacrificing the quality. This requires sophisticated automation pipelines in several stages.
  link: corpus
  